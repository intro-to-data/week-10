---
title: "Logistics"
output: html_notebook
---



# Setup

```{r include=FALSE}
pacman::p_load(gmodels, tidyverse)
titanic <- read_csv("data/titanic-train-clean.csv") |>
  mutate(died = !survived) |>
  select(-survived)
```


# Types of models

For our purposes, we can divide statistical models into two large camps.

1. Numeric
2. Categorical/Classification

And although intro to statistics classes focus on the former, the latter is every bit as important. Examples:

- Who is at risk of having a heart attack?
- Which credit card application is most likely to default?
- Which student is likely to graduate/drop-out?
- Which passenger is most likely to drown!



# A Titanic classification problem

- We will use a data set based on the sinking of the Titanic tonight.
- Can we figure out which passengers were most at risk of dying?
- Using what we know, what can we learn about the risk of dying aboard Titanic?

## A little history

- HMS Titanic sank on 15 April 1912 after "bumping" into an iceberg.
  - This accident would forever change Leonardo DiCaprio's life.
- In the early 20th century cruise ships did not carry enough lifeboats to save everyone.
- Lifeboats were used to shuttle passengers from a stricken ship to a rescue vessel.
  - They were not there to keep passengers/crew out of the ocean.
  - A stricken ship could be one with a mechanical breakdown, etc.
  - The horrible loss of life caused by the sinking of Titanic contributed to a changing attitude.
- I'm sure you have all heard the phrase "women and children first".
    - Tonight we will put this adage to the test.
    - Were women and children more likely to survive?

## Some R Skillz!

- Vectors - I haven't talked much about vectors.
- When we use the tidyverse, we can largely ignore them.
- But the code we need to write is MUCH easier to write using vectors.
- So, we shall use vectors.

```{r}
v_letters <- c("a", "b", "c", "d", "e")
v_numbers <- c(1, 2, 3, 4, 5)
v_short <- 1
```

- So yes, a variable, like v_short is actually a vector.
- R basically uses vectors to make up the columns in a data frame.
- And it provides some nifty syntax to help us use/filter these values.

```{r}
v_letters[3]
```
```{r}
v_numbers[3]
```

```{r}
v_letters[v_letters == "d"]
```

```{r}
v_numbers[v_numbers > 2]
```

```{r}
v_letters[v_numbers > 2]
```

And an easy way to get a vector out of a data frame is with the `$` operator.

```{r}
titanic$passengerid
```

```{r}
titanic |> filter(sex == "male") |> select(name)
```

```{r}
titanic$name[titanic$sex == "male"]
```


We will use this square bracket style filtering tonight.



# Exploring Titanic data

## What percentage of passengers died?

- Question: What percentage of passengers in this data set died?
- Answer: Roughly 62 percent of passengers died.

**Risk:** the number of passengers who died, divided by the total number of passengers.

This definition of risk is applicable beyond the macabre scope of sinking boats.

```{r}
## Look at this carefully.
titanic |>
  summarize(
    n_died = sum(died),
    n_passengers = n(),
    ## Note how I reuse n_died and n_passengers here.
    ## The round function just makes things easier to read.
    ## See ?round for more info.
    p_died = round(100*n_died/n_passengers,1)
  )
```

Here's what I'm doing with these columns above:

- n_died: The number of passengers who died. This is our numerator.
- n_passengers: The total number of passengers, regardless of if they died or not. This is our denominator!
- p_died: The percentage of passengers who died. Which is the numerator divided by the denominator.
  - This could also be referred to as the risk of dying.
  - Groups of passengers with a higher p_died have a higher risk of drowning.
  - For example, males. . . . 

## Were males more likely to die?

- Question: Was being male a risk factor for drowning?
- Answer: Everyone over the age of 12 knows this is YES, but can we prove it?

```{r}
## Reuse the code from above, but add a group_by statement.
titanic |>
  group_by(sex) |>
  summarize(
    n_died = sum(died),
    n_passengers = n(),
    p_died = round(100*n_died/n_passengers,1)
  )
```

- Over 80% of men died.
- Only one in four women died.
- Because men have a higher percentage died, they have the higher risk.

## Were adults more likely to drown?

- Question: Was being an adult a risk factor for drowning?
- Answer: Yes

Helpful Hint: The child column identifies children (1) and adults (0)

```{r}
titanic |>
  group_by(child) |>
  summarize(
    n_died = sum(died),
    n_passengers = n(),
    p_died = round(100*n_died/n_passengers,1)
  )
```

- Nearly 2/3 adults died.
- But less than half of the children on-board Titanic died.
- All of this goes to show there is some truth to the classic phrase about women and children first.
- It is interesting to me that being a female was over-all more protective than being a child.

And we were able to calculate all of this using the dplyr skills learned earlier.



# Risk v Odds

**Odds:** The number of passengers who died divided by the number of passengers who did not die.

Odds are, oddly, very similar to risk. But the denominator is different.

```{r}
## We are creating a new data set called ratios which we will use later.
ratios <-
  titanic |> 
  group_by(sex) |>
  summarize(
    died = sum(died),
    survived = n() - sum(died),
    total = n()
  ) |>
  mutate(
    risk_died = died/total,
    odds_died = died/survived
         )
ratios
```

Explanation:

- Risk: 81% of men died while only 25% of women died.
- Odds: For every man who survived, over four men died. For every 3 women who survived, one died (roughly).
- I did not convert risk into a percentage in the code for two reasons.
    1. I want you to compare risk to odds and I don't want to make it any more complicated than necessary.
    2. I want you to see how odds are higher than risk because the denominator is smaller. **THIS IS IMPORTANT**.
    3. I want to use these values directly to calculate risk/odds ratios.

```{r}
## This is why I made a new table called ratios.
## AND this is why we talked about the square bracket operator above.
ratios |>
  summarize(
    risk_died_female = risk_died[sex == "female"],
    risk_died_male = risk_died[sex == "male"],
    risk_died = risk_died[sex == "male"]/risk_died[sex == "female"],
    odds_died_female = odds_died[sex == "female"],
    odds_died_male = odds_died[sex == "male"],
    odds_died = odds_died[sex == "male"]/odds_died[sex == "female"]
  ) |>
  select(risk_died, odds_died)
```

- Because most passengers died, the odds ratio is larger than the risk ratio.
- When the outcome modeled is rare, these two converge.
- As shown below. In each of the four examples, 20 people died/got sick/whatever.
- When the total population is small (50) the odds and risk are different.
- But when the total population is (relatively) large, and the event is (relatively) rare, they converge.

| Died | Total | Risk | Odds |
|   20 |    50 |  .40 |  .67 |
|   20 |   100 |  .20 |  .25 |
|   20 |   200 |  .10 |  .11 |
|   20 |   500 |  .04 |  .04 | <-- **CONVERGENCE!!!**

Now we will develop a couple of SIMPLE classification "models".



# SIMPLE MODEL: Everyone dies!!!

A classification model is a method of predicting which group someone (or something) belongs to. In this case, we want to build a model to predict who will die aboard the Titanic. Thus, we want to classify passengers into one of TWO groups:

1. died (drowned)
2. survived (did not drown)

- Our first model "predicts" that everyone will die.
- It is a naive model, but it is also right over 60% of the time.
- This is a thought experiment, not as a serious exercise in modeling.

```{r}
# I kept the column name "pred".
titanic <- titanic |> mutate(pred = 1)
```


- We have 891 rows in `titanic`.
    - 549 people died, and our "model" predicts ALL of those deaths.
    - Our other 342 guesses are wrong.
- A classification model can be wrong in two ways.
  - **False Positive:** The model predicts a passenger died but they lived (beating the odds).
  - **False Negative:** The model predicts a passenger survived but they actually died.
- And when we put this into a table, we get a confusing thing called a confusion matrix.

| Actual   | Predicted Died       | Predicted Survived  |
|:--------:|:--------------------:|:-------------------:|
| Survived | FALSE POSITIVE (342) | TRUE NEGATIVE  (0)  |
| Died     | TRUE POSITIVE  (549) | FALSE NEGATIVE (0)  | 
| Total    |                (891) |                (0)  |

Because this first model predicts EVERYONE DIED, our model never has any TRUE or FALSE negatives.

- You want to find ways to maximize TRUE POSITIVE and TRUE NEGATIVE.
    - This classification model works by saying a member either belongs to the died group (positive) or that they survived (negative).
    - TRUE POSITIVE: The model predicted the passenger would die, and they did.
    - TRUE NEGATIVE: The model predicted the passenger would survive, and they did.
- Accuracy is the sum of TRUE POSITIVE + TRUE NEGATIVE / TOTAL
    - TRUE POSITIVE (549) + TRUE NEGATIVE (0) / TOTAL (891)
    - Which is 61.6 percent.

Let's build this model and the confusion matrix in R.

1. We will create a new column in titanic called predicted which we will then compare to died.
2. Because our first model predicts everyone dies, all values in predicted will be 1.
3. We will then use dplyr to build a confusion matrix.

```{r}
## This is our confusion matrix:
titanic |>
  group_by(died) |>
  summarize(
    predicted_died = sum(pred),
    predicted_survived = sum(!pred)
  )
```

- The labeling isn't quite as complex as above, but the numbers are identical.
- There is an easier way to do this:

```{r}
# This older R package feels funny if you are used to the tidyverse.
CrossTable(titanic$died, titanic$pred)
```

Although it doesn't look as nice. Both have their place.



# SIMPLE MODEL: All Women Survive & All Men Die

- Our data exploration earlier convincingly shows that women were more likely to survive than men.
- Our second model assumes ALL women live and ALL men die.
- And this time, we will only use R to do our work.

```{r}
## First, we make our prediction.
## This will overwrite our work above.
titanic <- 
  titanic |>
  mutate( 
    pred = case_when(sex == "male"~1, sex == "female"~0)
  )

## And then we assess our model.
## Note: This is the same code I wrote previously.
## But we are saving the results so we can reuse them.
confusion <- 
  titanic |>
  group_by(died) |>
  summarize(
    predicted_died = sum(if_else(pred == 1, 1, 0)),
    predicted_survived = sum(if_else(pred == 0, 1, 0))
  )
confusion
```

- Accuracy = TRUE POSITIVES + TRUE NEGATIVES / TOTAL
- .787 = (468 + 233) / 891
- A model that is nearly 80% accurate is actually pretty good.
- And we didn't even have to do any complicated math!
- How to calculate accuracy using R.

```{r}
confusion |> 
  summarize(
    accuracy = (predicted_died[died == 1] + predicted_survived[died == 0]) / nrow(titanic)
  )
```

- Models have to balance Type I and II error.
    - FALSE POSITIVE == Type  I Error
    - FALSE NEGATIVE == Type II Error
    - Frankly I cannot remember what Type I and Type II error even means.
    - Like most people, I refer to them as simply FALSE POSITIVE and FALSE NEGATIVE errors.
- All else being equal, you want to minimize both forms of error.

# Logistic Regression

- It is fine to calculate risk/odds for two populations and compare them by hand.
- Building models "by hand" is silly. And we won't do so again.

Instead, we can/should use a tool like logistic regression.

```{r}
## This is why I made a new table called ratios.
ratios |>
  summarize(
    risk_died_female = risk_died[sex == "female"],
    risk_died_male = risk_died[sex == "male"],
    risk_died = risk_died[sex == "male"]/risk_died[sex == "female"],
    odds_died_female = odds_died[sex == "female"],
    odds_died_male = odds_died[sex == "male"],
    odds_died = odds_died[sex == "male"]/odds_died[sex == "female"]
  ) |>
  select(risk_died, odds_died)
```

- This shows the odds males dying are larger than for females.
- The log of odds_died:

```{r}
## Just try to remember this number.
## It will come up in just a minute, I promise.
log(12.35066)
```


The function for a logistic model in R is `glm` which is an abbreviation of generalized linear model. Our formula is similar to what we did with linear regression. In this case we want to model died as a function of sex or died~sex.

```{r}
model_sex <- glm(died~sex, family = binomial, data = titanic)
summary(model_sex)
```

Look at the coefficients. They should look like:

```
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.0566     0.1290  -8.191 2.58e-16 ***
sexmale       2.5137     0.1672  15.036  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

The estimate for `sexmale` is 2.51. Now, look at this:

```{r}
exp(2.5137)
log(12.4)
```

An estimated "slope" of 2.5137 tells us that the odds of a male dying is roughly 12.4 times higher than the odds of a woman dying because this is our odds ratio. And remember, an odds ratio of roughly 1 would tell us that both groups have about the same amount of risk. And the log of 1 is 0. So if the estimate in our logistic regression model is 0 then there is no difference between the two groups. We can furthermore see that the p-value of this feature is less that .05 which should not surprise you.

And we can use this model to classify passengers into died/survived. If you look at the predicted values from our model_sex, there are only two values .811 and .258.

```{r}
## head gives you the first six values from a list
head(model_sex$fitted.values)
```

Now, if you recall:

```{r}
titanic |>
  group_by(sex) |>
  summarize(
    n_died = sum(died),
    n_passengers = n(),
    p_died = n_died/n_passengers
  )
```

- I hope you see the pattern here.
- The fitted values are the risks we calculated earlier.
- To recreate our model where we have all the women live and all the men die, we could do the following:

```{r}
## This creates a new column called fitted which has the fitted values from the model.
titanic$fitted <- model_sex$fitted.values


titanic <- 
  titanic |>
  mutate(
    ## This is what we did last time.
    ##pred = case_when(sex == "male"~1, TRUE~0)

    ## But here's a different way to do it.
    ## If the passenger has a better than 50% chance of dying,
    ## we will classify them as dead, otherwise, they survive.
    pred = case_when(fitted >= .5~1, TRUE~0)
  )

## And then we assess our model.
## Note: This is the same code I wrote previously.
## But we are saving the results so we can reuse them.
titanic |>
  group_by(died) |>
  summarize(
    predicted_died = sum(if_else(pred == 1, 1, 0)),
    predicted_survived = sum(if_else(pred == 0, 1, 0))
  )
```

And this gives us the EXACT SAME confusion matrix as before, but we did it with the results of our logistic regression.

The advantage of using a logistic regression model is that, as you can see in the summary gives us more diagnostic information and we can easily build more complex models.
